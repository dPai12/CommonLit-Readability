---
title: "DS2_Project"
author: "Divya Pai"
output: html_document
---

```{r setup, include=FALSE}
require(tidyverse, quietly = TRUE)
library(tidymodels)
library(tidytext)
library(dplyr)
library(parsnip)
library(keras)
library(ggplot2)
library(textrecipes)
```

```{r}
install.packages("udpipe")
```
```{r}
install.packages("quanteda")
```

```{r}
library(udpipe)
library(lattice)
```
```{r}

```

```{r}
commonlit_training = read.csv("/Users/kamathshashwath/Desktop/Data Sc 2/commonlitreadabilityprize/train.csv")
commonlit_testing = read.csv("/Users/kamathshashwath/Desktop/Data Sc 2/commonlitreadabilityprize/test.csv")
commonlit_ss = read.csv("/Users/kamathshashwath/Desktop/Data Sc 2/commonlitreadabilityprize/sample_submission.csv")
```

```{r}
commonlit_training
```

```{r}
lit_training = commonlit_training %>%
  select(-url_legal,-license,-standard_error)
```

```{r}
## Tokens 
training_tok = lit_training %>% unnest_tokens(word,excerpt)
```

```{r}
## Token count per id
training_tok_n = training_tok %>% count(id)
```
```{r}
df_training = lit_training %>% 
  select(id,target) %>%
  left_join(training_tok_n, by = "id") %>%
  rename(n_token = n)
```
```{r}
training_sent = lit_training %>%
  unnest_tokens(output = sentence, input = excerpt, token = "sentences") %>% count(id)
training_sent

```
```{r}
df_training = df_training %>%
  left_join(training_sent, by="id") %>%
  rename(n_sentence = n)
```

```{r}
## count stop words
training_filt = training_tok %>% filter(!word %in% stop_words$word)
training_filt_n = training_filt %>%
  count(id)
df_training = df_training %>%
  left_join(training_filt_n, by = "id") %>% 
  rename(n_stop=n) %>% 
  mutate(stop_prop=n_stop/n_token)
```

```{r}
training_filt_l = training_filt %>% mutate(l = str_length(word)) %>%
  group_by(id) %>%
  summarise(word_len = sum(l),
            avg_word_len = mean(l))
```

```{r}
training_filt_l
```
```{r}
df_training = df_training %>%
  left_join(training_filt_l, by = "id")
```

```{r}
model = udpipe_download_model(language = "english")
str(model)
```
```{r}
udmodel_english <- udpipe_load_model(model$file_model)
```
```{r}
s = udpipe_annotate(udmodel_english, training_filt$word)
x = data.frame(s)
```

```{r}
stats = subset(x, upos %in% c("NOUN"))
stats_noun = txt_freq(stats$token)
stats
stats_noun
```
```{r}
stats1 <- subset(x, upos %in% c("VERB"))
stats_verb <- txt_freq(stats1$token)
stats2 <- subset(x, upos %in% c("ADJ"))
stats_adj <- txt_freq(stats2$token)
```


## a is a list of all nouns in training_filt. Now we need to get count of unique nouns per id. 
```{r}
filt_noun = training_filt %>% filter(word %in% stats_noun$key)
filt_noun
```
## b is the number of unique nouns per id.
```{r}
unique_noun_n = filt_noun %>%
  distinct(id, word) %>%
  count(id) %>%
  rename(unique_noun_n = n)
unique_noun_n
```
```{r}
filt_verb = training_filt %>% filter(word %in% stats_verb$key)
unique_verb_n = filt_verb %>%
  distinct(id, word) %>%
  count(id) %>%
  rename(unique_verb_n = n)
unique_verb_n
```

```{r}
df_training1 = df_training %>%
  left_join(unique_noun_n, by = "id") %>%
  left_join(unique_verb_n, by = "id") 
  
```
```{r}
df_training1
```
```{r}
df_training1 %>%
  select_if(is.numeric) %>%
  cor()
```
## Linear Regression Model
## Split data with strata target
```{r}
set.seed(1234)
l_split = initial_split(df_training1, strata = target)
l_training = training(l_split)
l_testing = testing(l_split)
```

```{r}
lit_mod = linear_reg() %>%
  set_engine('lm') %>%
  set_mode('regression')
```

```{r}
lit_rec = recipe(target~., data = l_training)  %>%
  step_rm(id) %>%
  step_corr(all_numeric_predictors() , threshold = 0.9) %>%
  step_nzv(all_predictors())
```
```{r}
lit_wflow = workflow() %>%
  add_model(lit_mod) %>%
  add_recipe(lit_rec)
```
## fit the model and training data
```{r}
lit_fit = lit_wflow %>%
  fit(data = l_training)
```

## results
```{r}
l_training_results = predict(lit_fit, l_training) %>%
  bind_cols(l_training %>% select(target))
```

```{r}
l_training_results
```
# 
```{r}
rsq(l_training_results, truth = target, estimate = .pred)
```
```{r}
rmse(l_training_results, truth = target, estimate = .pred)
```
```{r}
l_testing_results = predict(lit_fit, l_testing) %>%
  bind_cols(l_testing %>% select(target))
```

```{r}
rsq(l_testing_results, truth = target, estimate = .pred)
rmse(l_testing_results, truth = target, estimate = .pred)
```
## SVM Model
```{r}
s_df = lit_training %>%
left_join(df_training1, by = ("id")) %>%
select(-target.x) %>%
rename(target = target.y)
```

```{r}
set.seed(1234)
s_split = initial_split(s_df, prop = 0.75)
s_training = training(s_split)
s_testing = testing(s_split)
```

```{r}
s_model = svm_poly(degree = 2, cost = 0.1) %>%
            set_engine('kernlab')%>%
            set_mode('regression')%>%
            translate()
```

```{r}
s_recipe = recipe(target~ . , data=s_training)%>%
            step_rm(id)%>%
            step_tokenize(excerpt)%>%
step_tokenfilter(excerpt, max_tokens = 100) %>%
step_tfidf(excerpt) %>%
step_corr(all_numeric_predictors() , threshold = 0.9) %>%
            step_normalize(all_numeric_predictors())
```

```{r}
s_wflow = workflow()%>%add_model(s_model)%>%
                           add_recipe(s_recipe)
```

```{r}
s_fit = s_wflow %>% fit(data=s_training)
```

```{r}
predict(s_fit, s_training)
```

```{r}
s_test_res = predict(s_fit, new_data = s_testing) %>%
  bind_cols(s_testing %>% select(target))
s_test_res
```
```{r}
metrics(s_test_res, truth=target, estimate=.pred)
```
```{r}
s_model2 = svm_poly(degree=tune(), cost=tune(), scale_factor=tune())%>%
            set_engine('kernlab')%>%
            set_mode('regression')%>%
            translate()
```


```{r}
s_grid = expand.grid(cost=c(0.001, 0.1, 0.5, 1 , 10 ), degree=c(1,2,3,4,5), scale_factor=c(.1,.5,1))
```

```{r}
samples = vfold_cv(s_training, v=5, strata= target)
```


```{r}
s_wflow2 = workflow()%>%
  add_recipe(s_recipe)%>%
  add_model(s_model2)
```

```{r}
s_tuning = s_wflow2 %>%
   tune_grid(resamples = samples, grid= s_grid)
```

```{r}
s_tuning %>% show_best('rmse')
```

```{r}
s_best = s_tuning %>% select_best(metric='rmse')
```

```{r}
final_s_wflow = s_wflow2 %>% finalize_workflow(s_best) %>% fit(s_training)
```

```{r}
last_fit_svm = final_s_wflow%>% 
  last_fit(split=s_split)
```

```{r}
final_test_results =last_fit_svm%>%collect_metrics()
```
```{r}
final_test_results
```

```{r}
## Prep test data
lit_testing = commonlit_testing %>%
  select(-url_legal,-license)

## Tokens 
testing_tok = lit_testing %>% unnest_tokens(word,excerpt)

## Token count per id
testing_tok_n = testing_tok %>% count(id)

df_testing = lit_testing %>% 
  select(id) %>%
  left_join(testing_tok_n, by = "id") %>%
  rename(n_token = n)
```

```{r}
testing_sent = lit_testing %>%
  unnest_tokens(output = sentence, input = excerpt, token = "sentences") %>% count(id)

df_testing = df_testing %>%
  left_join(testing_sent, by="id") %>%
  rename(n_sentence = n)
```

```{r}
## count stop words
testing_filt = testing_tok %>% filter(!word %in% stop_words$word)
testing_filt_n = testing_filt %>%
  count(id)
df_testing = df_testing %>%
  left_join(testing_filt_n, by = "id") %>% 
  rename(n_stop=n) %>% 
  mutate(stop_prop=n_stop/n_token)

```

```{r}
model = udpipe_download_model(language = "english")
udmodel_english = udpipe_load_model(model$file_model)

```

```{r}
test_a = udpipe_annotate(udmodel_english, testing_filt$word)
test_ud = data.frame(test_a)

test_noun = subset(test_ud, upos %in% c("NOUN"))
test_noun_freq = txt_freq(test_noun$token)

test_verb = subset(test_ud, upos %in% c("VERB"))
test_verb_freq = txt_freq(test_verb$token)
```


```{r}
test_filt_noun = testing_filt %>% filter(word %in% test_noun_freq$key)
```

```{r}
test_unique_noun = test_filt_noun %>%
  distinct(id, word) %>%
  count(id) %>%
  rename(unique_noun_n = n)
test_unique_noun
```
```{r}
test_filt_verb = testing_filt %>% filter(word %in% test_verb_freq$key)
test_unique_verb = test_filt_verb %>%
  distinct(id, word) %>%
  count(id) %>%
  rename(unique_verb_n = n)
test_unique_verb
```
```{r}
df_testing1 = df_testing %>%
  left_join(test_unique_noun, by = "id") %>%
  left_join(test_unique_verb, by = "id") 
```

```{r}
testing_filt_l = testing_filt %>% mutate(l = str_length(word)) %>%
  group_by(id) %>%
  summarise(word_len = sum(l),
            avg_word_len = mean(l))
```
```{r}
df_testing1 = df_testing1 %>%
  left_join(testing_filt_l, by = "id")
```
```{r}
final_test = lit_testing %>%
left_join(df_testing1, by = "id")
final_test
```

```{r}
test_preds = predict(final_s_wflow, final_test)
```
```{r}
commonlit_ss$target = test_preds$.pred
```

```{r}
commonlit_ss
```







